{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87d7d905",
   "metadata": {},
   "source": [
    "# Part 1: Stats and Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b75e201",
   "metadata": {},
   "source": [
    "Problem: You want to try to determine who is going to win an upcoming election.  To do so, you’re going to poll some number of people to try to estimate the probability a given voter supports Candidate A over Candidate B (we will assume no one is indifferent between them)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2d0fbb",
   "metadata": {},
   "source": [
    "#### 1)  If  the  probability  a  person  supports  Candidate A is p,  how  many  people  should  you  poll  to  try  to  be  95% confident you know the value of p to within an error of 0.01?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6cbb75",
   "metadata": {},
   "source": [
    "We have a random variable X~Bern(p) that represents the likelihood that a person will vote for candidate A, depending on i.i.d. RVs X1, X2, ... Xn. We're interested in the probability that the difference between sample mean X bar n and the true value of p is below a certain threshold, i.e. 0.01 in this case. So, we can use the Hoeffding inequality, and set the upper bound of failure to be less than 5%.\n",
    "\n",
    "P(|X bar n - p| >= ep) <= 2e^(-2N(ep)^2) <br />\n",
    "P(|X bar n - p| >= 0.01) <= 0.05 <br />\n",
    "2e^(-2N(0.01)^2) <= 0.05 <br />\n",
    "-2N(0.01)^2 <= ln(0.025) <br />\n",
    "-0.0002N <= ln(0.025) <br />\n",
    "N >= ln(0.025)/-0.0002 = 18444.39 <br />\n",
    "\n",
    "We should try to poll at least 18445 people to be 95% confident we know p to within an error of 1%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ad044c",
   "metadata": {},
   "source": [
    "#### 2)  What if you can only afford to poll k = 30 people - how accurately can you say you know p (i.e., what error do you know p to) with 95% confidence?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ec0de0",
   "metadata": {},
   "source": [
    "Once again, we can use the Hoeffding inequality, with N=30 and the right hand side being 0.05.<br /> <br />\n",
    "P(|X bar n - p| >= ep) <= 2e^(-2N(ep)^2) <br />\n",
    "P(|X bar 30 - p| >= ep) <= 0.05 <br />\n",
    "2e^(-2* 30*(ep)^2) <= 0.05 <br />\n",
    "-60(ep)^2 <= ln(0.025) <br />\n",
    "ep^2 >= 0.06148 <br />\n",
    "ep >= 0.248\n",
    "\n",
    "We can know p within an error of 0.248. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1d77af",
   "metadata": {},
   "source": [
    "#### 3)  Generate synthetic data by sampling a Bernoulli(0.55) distribution k= 30 times.  What did you get for  ˆp, and how does it compare with p=0.55?  Does this seem consistent with the previous answers?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5426e3e1",
   "metadata": {},
   "source": [
    "For this question, we'll use some python code to generate this sample mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5590351a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "random.seed(1000) # makes sure the values don't change every time the cell is run\n",
    "\n",
    "def sample30():\n",
    "    arr = []\n",
    "    for i in range(30):\n",
    "        x = np.random.uniform(0.0, 10.0)\n",
    "        if x<=5.5:\n",
    "            arr.append(1)\n",
    "        else:\n",
    "            arr.append(0)\n",
    "    samplemean = sum(arr)/len(arr)\n",
    "    return samplemean\n",
    "\n",
    "print(sample30())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8dcf48c",
   "metadata": {},
   "source": [
    "We get a sample mean of 0.466, which seems pretty far off from the true mean p = 0.55. However, as we needed to poll over 18000 to be relatively confident in getting close to p, and in problem 2 the epsilon value calculated is large enough to fit in 0.46 confidently, it makes sense. To see if this is consistent with my answer to problem 2,  let's calculate the sample mean some number of times to calculate the probability it will diverge from p by my calculated error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "492f3e0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9687\n",
      "0.967\n",
      "0.9682\n",
      "0.9662\n",
      "0.9676\n",
      "0.9688\n",
      "0.9674\n",
      "0.9683\n",
      "0.9681\n",
      "0.9672\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(1000)\n",
    "\n",
    "p = 0.466\n",
    "error = 0.248\n",
    "\n",
    "trials = 10000\n",
    "\n",
    "for j in range(10): # Testing 10 times\n",
    "    successes = 0\n",
    "    for i in range(trials): # each test sees what the rate of success is for the sample mean being inside the error range\n",
    "        if abs(sample30()-p) < error: # each trial checks if the sample mean and true mean are within the error range\n",
    "            successes+=1\n",
    "    bound = successes/trials\n",
    "    print(bound)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f3d543",
   "metadata": {},
   "source": [
    "After running this simulation 10 times, it seems clear that we can be around 96% sure that this specific sample mean is within the error bound. This is consistent. 96 percent is better than the calculated 95 even for a sample mean that seemed far off from the population mean. It's also not too much better: the Hoeffding inequality is supposed to be a tight bound, so the calculations seem fine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30bdfd39",
   "metadata": {},
   "source": [
    "#### 4)  If N people vote in the election, each with a probability p of voting for Candidate A, then the number of votes Candidate A  receives will be random. What is the distribution of the number of votes Candidate A receives?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f254575d",
   "metadata": {},
   "source": [
    "In this problem, we are interested in the probability distribution of the number of successes in N independent trials, i.e. whether or not a given person votes for Candidate A, each with probability of success p. Therefore, its clear that the number of votes can be modeled with a binomial distribution (each person is a separate Bernoulli trial) with parameters N and p. Namely, if we name this random variable A, then P_A(a) = (N choose a)(p^a)(1-p)^(N-a)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6b5b54",
   "metadata": {},
   "source": [
    "#### 5)  Assume 1000 people vote.  If p = 0.55, as above, what is the probability that candidate A wins the election? (i.e., receives a majority of the votes). Be clear on how you are calculating this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a25256",
   "metadata": {},
   "source": [
    "To find out the probability that candidate A wins, we need to calculate the probability that he receives 501 votes or more. Because the random variable A~Binomial(1000, 0.55) is discrete, we can use the sum from a=501 to a=1000 of P_A = (N choose a)(p^a)(1-p)^(N-a), or equivalently, 1 - the CDF of P_A of A = 500. As calculating the sum manually is very expensive, I can  simply import a function to calculate this quantity much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a1621a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9991534507833808"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import binom\n",
    "1 - binom.cdf(500, 1000, 0.55)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee82b25",
   "metadata": {},
   "source": [
    "So, it is practically 100% guaranteed that candidate A is going to win. To make sure this makes sense, we can look at the  distribution graphically below. Notice how basically all of the data is greater than A=500."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a36521e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\neela\\anaconda3\\lib\\site-packages\\seaborn\\distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from numpy import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "random.seed(1000)\n",
    "\n",
    "sns.distplot(random.binomial(n=1000, p=0.55, size=1000))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7eb5297",
   "metadata": {},
   "source": [
    "#### 6)  If you estimate p with ˆp based on your data, what is the probability that candidate A wins the election? Does this seem consistent with the true probability?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bdafc1a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.014444001194606093"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samplep = 0.466\n",
    "1 - binom.cdf(500, 1000, samplep)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ecd0178",
   "metadata": {},
   "source": [
    "The probability has dropped dramatically to just above a 1% chance of winning. While alarming at first, this is only the result of one sample mean, and 0.466 is relatively far from p=0.55. It's clear that its around p=0.5 where the cdf of the binomial distribution is most sensitive (as the derivative of the cdf is simply the binomial pdf, which has a maximum at p=0.5), so switching to the other side makes candidate A much more likely to lose, even more so with a larger population voting (n value)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbac38c9",
   "metadata": {},
   "source": [
    "#### 7)  Generate a ‘guess’ for the value of p by sampling a N(ˆp,ˆp(1−ˆp)/k) distribution. Using this guess at p, compute the  probability  that  Candidate A wins  the  election. Do  this 1000 times, and average the probabilities of Candidate A winning, to a final estimate for the probability that Candidate A wins. What do you get, and how does it compare with just using ˆp by itself?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4769aac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19394129049438294\n",
      "0.3535303270877649\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import math\n",
    "random.seed(1000)\n",
    "\n",
    "samplep = 0.466\n",
    "k = 30\n",
    "stddev = math.sqrt((samplep*(1-samplep))/k)\n",
    "\n",
    "guesses = np.random.normal(loc=samplep, scale=stddev, size=(1,1000)) # generates the 1000 guesses for the true p\n",
    "guesses = guesses[0] # has to do with the way numpy formats its arrays\n",
    "\n",
    "# print(stddev)\n",
    "probabilityarray = []\n",
    "\n",
    "for guess in guesses:\n",
    "    prob = 1-binom.cdf(500, 1000, guess)\n",
    "    probabilityarray.append(prob)\n",
    "\n",
    "print(np.var(np.array(probabilityarray)))\n",
    "print(sum(probabilityarray)/len(probabilityarray))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e62efd",
   "metadata": {},
   "source": [
    "Sampling the distribution using 30 people, running this simulation 1000 times and averaging out the result, we get that Candidate A only has 34% chance of a win, far off from both the results of 100% and 1% from the previous calculations. However, looking at probabilityarray itself, there's a wide variety of probabilities. This again comes from the fact that 30 is a relatively small sample size in this case, meaning the variance of the sampling distribution ˆp(1−ˆp)/k) is going to be large. We are dealing with errors from both the normal sampling distribution as well as the sample itself. Indeed, increasing the k value reduces variance in the array, and produces a more stable output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a47286f",
   "metadata": {},
   "source": [
    "#### Bonus)  In the 1948 US Presidential election, many papers and pollsters called the election for Thomas Dewey, but the results later showed Truman won by a significant margin.  This is the source of the infamous, incorrect headline ‘DEWEY DEFEATS TRUMAN’. Why were the predictions so wrong?  Be thorough. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc682da",
   "metadata": {},
   "source": [
    "The main reason of these incorrect polling results were because of a flawed method called quota sampling, where polling firms in 1948 would try to poll a certain number of people from each demographic in order to try to get a representative sample of the population. However, as pollers had freedom on who and where to poll, they ended up mostly going to more affluent neighborhoods, for ease of access. As Republicans were more wealthy on average than Democrats around that time, the sample ended up being biased anyway. This is highly unlike the probabilistic polling that we've done in this problem, where polls are completely random. Quota sampling is akin to having the Bernoulli variables that we were basing our sample mean on being dependent on each other rather than i.i.d, which would definitely skew the results. People who live near each other or in the same neighborhood are definitely likely to identify similarly when it comes to politics. To eliminate such intrinsic biases, this is why generating data from a random distribution is ideal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44b0ac8",
   "metadata": {},
   "source": [
    "# Part 2: Regression Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46aa502f",
   "metadata": {},
   "source": [
    "#### Generate synthetic data..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8a6a40",
   "metadata": {},
   "source": [
    "#### 1)  If you had to model Y as a constant value, i.e., f(x) = c, based on your data, what value c should you pick? Why? What is the error for the best c for your data?  How does the error on the training vs testing set differ? Does the value of d matter? Why or why not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ed08a4",
   "metadata": {},
   "source": [
    "For this problem, as the output values of vector y don't depend on features X6 through Xd, lets omit them for the constant model to simplify our dataset a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2ea915c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bernoulli(p, trials):\n",
    "    arr = []\n",
    "    for i in range(trials):\n",
    "        x = random.randint(1,10)\n",
    "        if x<=p*10:\n",
    "            arr.append(1)\n",
    "        else:\n",
    "            arr.append(0)\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a81160f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.685484</td>\n",
       "      <td>-1.652540</td>\n",
       "      <td>-0.619596</td>\n",
       "      <td>0.120728</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-21.179914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.780068</td>\n",
       "      <td>-2.723338</td>\n",
       "      <td>-0.666609</td>\n",
       "      <td>0.523218</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-67.766190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.970291</td>\n",
       "      <td>-1.454738</td>\n",
       "      <td>0.060815</td>\n",
       "      <td>0.297310</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-22.706263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.888535</td>\n",
       "      <td>-0.491324</td>\n",
       "      <td>1.905886</td>\n",
       "      <td>2.276102</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-19.312170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.392765</td>\n",
       "      <td>-1.109058</td>\n",
       "      <td>-0.825350</td>\n",
       "      <td>0.793778</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-4.735151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>2.848894</td>\n",
       "      <td>-1.732607</td>\n",
       "      <td>-0.616321</td>\n",
       "      <td>0.071499</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-20.791479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>3.719160</td>\n",
       "      <td>-1.011943</td>\n",
       "      <td>1.695275</td>\n",
       "      <td>0.976257</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-36.405232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>2.675946</td>\n",
       "      <td>-3.212153</td>\n",
       "      <td>-3.748361</td>\n",
       "      <td>1.469316</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-23.797072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>2.654923</td>\n",
       "      <td>-2.769379</td>\n",
       "      <td>-2.883836</td>\n",
       "      <td>0.591945</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-22.265848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>2.226243</td>\n",
       "      <td>-1.509134</td>\n",
       "      <td>-0.792025</td>\n",
       "      <td>0.240950</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-12.259169</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            x1        x2        x3        x4   x5          y\n",
       "0     2.685484 -1.652540 -0.619596  0.120728  1.0 -21.179914\n",
       "1     4.780068 -2.723338 -0.666609  0.523218  1.0 -67.766190\n",
       "2     2.970291 -1.454738  0.060815  0.297310  0.0 -22.706263\n",
       "3     2.888535 -0.491324  1.905886  2.276102  1.0 -19.312170\n",
       "4     1.392765 -1.109058 -0.825350  0.793778  1.0  -4.735151\n",
       "...        ...       ...       ...       ...  ...        ...\n",
       "9995  2.848894 -1.732607 -0.616321  0.071499  0.0 -20.791479\n",
       "9996  3.719160 -1.011943  1.695275  0.976257  1.0 -36.405232\n",
       "9997  2.675946 -3.212153 -3.748361  1.469316  1.0 -23.797072\n",
       "9998  2.654923 -2.769379 -2.883836  0.591945  1.0 -22.265848\n",
       "9999  2.226243 -1.509134 -0.792025  0.240950  1.0 -12.259169\n",
       "\n",
       "[10000 rows x 6 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "random.seed(10000)\n",
    "\n",
    "mu1, mu2, mu6 = 3, -2, 0\n",
    "sigma = 1\n",
    "errorfory = np.random.normal(0, np.sqrt(sigma), (11000,1))\n",
    "\n",
    "# generating dataset from features\n",
    "x1 = np.random.normal(mu1, sigma, (11000,1))\n",
    "x2 = np.random.normal(mu2, sigma, (11000,1))\n",
    "x3 = x1 + 2*x2\n",
    "x4 = (x2 + 2)**2\n",
    "x5 = np.array(bernoulli(0.8, 11000)).reshape(11000,1)\n",
    "ycol = 4-(3*x1**2)+x3-(0.01*x4)+(x2*x5)+errorfory\n",
    "\n",
    "# putting them together\n",
    "x = np.stack([x1,x2,x3,x4,x5,ycol], axis=1).reshape(11000,6)\n",
    "\n",
    "# view data table\n",
    "df = pd.DataFrame(data=x, columns=['x1', 'x2', 'x3', 'x4', 'x5', 'y'])\n",
    "# df\n",
    "\n",
    "# split into train and test sets\n",
    "df_train = df[0:10000]\n",
    "df_test = df[10000:].reset_index(drop=True)\n",
    "\n",
    "df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219b5610",
   "metadata": {},
   "source": [
    "Intuitively, it makes sense that if we were modelling y as a constant value, we should make that constant value equal to the mean of all y values in the training dataset, as it best represents the center of the data in terms of smallest difference from every number on average. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f6835a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-28.90444505701138\n",
      "14.022023302445701\n",
      "14.217032202394199\n",
      "2.628046590424467 -139.5462489966199\n"
     ]
    }
   ],
   "source": [
    "constantval = df_train['y'].mean() # mean of y values\n",
    "\n",
    "trainerror = 0\n",
    "testerror = 0\n",
    "for i in range(df_train['y'].shape[0]): # iterate down the y column\n",
    "    trainerror += abs(df_train.iloc[i,-1] - constantval) \n",
    "\n",
    "for i in range(df_test['y'].shape[0]): # iterate down the y column\n",
    "    testerror += abs(df_test.iloc[i,-1] - constantval) \n",
    "    \n",
    "# print out the average error per example\n",
    "print(constantval)\n",
    "print(trainerror/10000)\n",
    "print(testerror/1000)\n",
    "\n",
    "print(df_test['y'].max(), df_test['y'].min())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80e0021",
   "metadata": {},
   "source": [
    "The average error for the training set is 14.0, and for the test set it's 14.2. This makes sense for a set that ranges from around -139 to 2, and it'll serve as a benchmark for the other two models. In this case, the value of d will not affect the model's performance at all. As this model is hand picked by us to be the nothing more than the mean of the y values, there is no learning going on. The y vector does not depend on X6 through Xd, meaning the mean of its values does not depend on X6 through Xd either."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38419f01",
   "metadata": {},
   "source": [
    "#### 2)  Write a program to take a data set and fit a decision tree to it.  For d = 10, generate a plot of the decision tree error on the training and testing data as the depth of the tree you’re fitting increases.  What appears to be the optimal depth to grow the tree to to minimize the error?  How does the performance of this tree compare to the performance of the constant model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45e4316",
   "metadata": {},
   "source": [
    "First, let's implement the decision tree algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e3575545",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "maxdepth = 5\n",
    "minsamplesize = 10\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, left, right, depth, dataframe):\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        self.dataframe = dataframe\n",
    "        self.feature = None\n",
    "        self.featurecoeff = -2\n",
    "        self.alpha = None\n",
    "        self.alphaerror = None\n",
    "        self.depth = depth\n",
    "    \n",
    "    def splitNode(self, dataframe, y_vector): # recursive: runs through the entire subtree when called on node \n",
    "        \n",
    "        if dataframe.shape[0] == 1 or dataframe.shape[0] == 0: # if there's only one datapoint in the node \n",
    "            return\n",
    "        \n",
    "        # choose feature to split on\n",
    "        for feature in dataframe.columns:\n",
    "            print(f'{feature} --- {self.correlationcoeff(dataframe[feature], y_vector)}')\n",
    "            if self.correlationcoeff(dataframe[feature], y_vector) > self.featurecoeff:\n",
    "                self.feature = feature\n",
    "                self.featurecoeff = self.correlationcoeff(dataframe[feature], y_vector)\n",
    "                \n",
    "        # choose alpha to split on\n",
    "        alpha_array = []\n",
    "        for i in range(dataframe[self.feature].shape[0]-1):\n",
    "            midpoint = (dataframe[self.feature][i]+dataframe[self.feature][i+1])/2\n",
    "            alpha_array.append(midpoint)\n",
    "        for alpha in alpha_array:\n",
    "            weightederror = self.alphatotalerror(dataframe, y_vector, self.feature, alpha)\n",
    "            if self.alphaerror is None:\n",
    "                self.alphaerror = weightederror\n",
    "                self.alpha = alpha\n",
    "            elif weightederror < self.alphaerror:\n",
    "                self.alphaerror = weightederror\n",
    "                self.alpha = alpha\n",
    "                \n",
    "        \n",
    "        print(self.alpha, self.alphaerror)\n",
    "        \n",
    "        # split datasets \n",
    "        dataframebig = pd.concat([dataframe,y_vector], axis=1)\n",
    "        df_left = dataframebig.loc[dataframe[self.feature]<alpha] # split below threshold\n",
    "        df_right = dataframebig.loc[dataframe[self.feature]>=alpha] # split above threshold\n",
    "        \n",
    "        df_leftx = df_left.iloc[0,0:-1]\n",
    "        df_lefty = df_left.iloc[:,-1]\n",
    "        df_rightx = df_right.iloc[0,0:-1]\n",
    "        df_righty = df_right.iloc[:,-1]\n",
    "        \n",
    "        self.left = Node(left=None, right=None, dataframe=df_left, depth=self.depth+1) # create new node to the left\n",
    "        self.right = Node(left=None, right=None, dataframe=df_right, depth=self.depth+1) # create new node to the right\n",
    "        \n",
    "        # recurse down successive nodes given certain conditions\n",
    "#         if self.left.depth < maxdepth and df_leftx.shape[0]>1 and df_lefty.shape[0]>1:\n",
    "#             self.left.splitNode(df_leftx, df_lefty)\n",
    "#         if self.right.depth < maxdepth and df_rightx.shape[0]>1 and df_righty.shape[0]>1:\n",
    "#             self.right.splitNode(df_rightx, df_righty)\n",
    "        \n",
    "        return (df_leftx, df_lefty, df_rightx, df_righty)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def correlationcoeff(self, x_column, y_outputvector): # calculates correlation coefficient for one feature\n",
    "        x_column = x_column.to_numpy()\n",
    "        \n",
    "        m = x_column.shape[0] # number of training examples\n",
    "\n",
    "        exy = 0\n",
    "        ex = 0\n",
    "        ey = 0\n",
    "        exsquare = 0\n",
    "        eysquare = 0\n",
    "\n",
    "        for j in range(m-1):\n",
    "            exy += x_column[j]*y_outputvector[j]\n",
    "            ex += x_column[j]\n",
    "            ey += y_outputvector[j]\n",
    "            exsquare += x_column[j]**2\n",
    "            eysquare += y_outputvector[j]**2\n",
    "        exy = exy/m\n",
    "        ex = ex/m\n",
    "        ey = ey/m\n",
    "        exsquare = exsquare/m\n",
    "        eysquare = eysquare/m\n",
    "        \n",
    "        covariance = exy-(ex*ey)\n",
    "        varx = exsquare-(ex**2)\n",
    "        vary = eysquare-(ey**2)\n",
    "        \n",
    "        corrxy = covariance/(np.sqrt(varx*vary))\n",
    "        return corrxy\n",
    "        \n",
    "    \n",
    "    def alphatotalerror(self, dataframe, y_output, feature, alpha): # calculates error of one alpha given a feature\n",
    "#         print(\"..\")\n",
    "        m_data = y_output.shape[0] # number of training examples\n",
    "        \n",
    "        dataframeplusyvector = pd.concat([dataframe,y_output], axis=1)\n",
    "        \n",
    "        dataframeplusyvector_left = dataframeplusyvector.loc[dataframeplusyvector[feature]<alpha] # split below threshold\n",
    "        dataframeplusyvector_right = dataframeplusyvector.loc[dataframeplusyvector[feature]>alpha] # split above threshold\n",
    "        \n",
    "        \n",
    "        df_xleft = dataframeplusyvector_left.iloc[:, 0:-1]\n",
    "        df_yleft = dataframeplusyvector_left.iloc[:, -1]\n",
    "        \n",
    "        df_xleft.reset_index(drop=True, inplace=True)\n",
    "        df_yleft.reset_index(drop=True, inplace=True)\n",
    "        \n",
    "        df_xright = dataframeplusyvector_right.iloc[:, 0:-1]\n",
    "        df_yright = dataframeplusyvector_right.iloc[:, -1]\n",
    "        \n",
    "        df_xright.reset_index(drop=True, inplace=True)\n",
    "        df_yright.reset_index(drop=True, inplace=True)\n",
    "        \n",
    "        m_leftdata = df_yleft.shape[0]\n",
    "        m_rightdata = df_yright.shape[0]\n",
    "        \n",
    "        df_xleft = df_xleft.to_numpy()\n",
    "        df_yleft = df_yleft.to_numpy()\n",
    "        df_xright = df_xright.to_numpy()\n",
    "        df_yright = df_yright.to_numpy()\n",
    "\n",
    "        error_left = 0\n",
    "        error_right = 0\n",
    "        for i in range(m_leftdata): # iterate over left side examples\n",
    "            error_i = df_yleft.mean() - df_yleft[i]\n",
    "            error_left += error_i\n",
    "        \n",
    "        for j in range(m_rightdata): # iterate over right side examples\n",
    "            error_j = df_yright.mean() - df_yright[j]\n",
    "            error_right += error_j\n",
    "    \n",
    "        error_left = error_left / m_leftdata # average error on left\n",
    "        error_right = error_right / m_rightdata\n",
    "        \n",
    "        weightederror = ((m_leftdata/m_data)*error_left)+((m_rightdata/m_data)*error_right)\n",
    "        \n",
    "        return weightederror\n",
    "\n",
    "    \n",
    "    def traverse(self, x_test_row, y_val):\n",
    "        if self.left is None and self.right is None:\n",
    "            mean = self.dataframe[-1].mean()\n",
    "            return y_val-mean\n",
    "        \n",
    "        if x_test_row[f'{self.feature}'] < self.alpha:\n",
    "            traverse(self.left, x_test_row)\n",
    "        elif x_test_row[f'{self.feature}'] >= self.alpha:\n",
    "            traverse(self.right, x_test_row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "48bb9218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1 --- -0.9550569577881499\n",
      "x2 --- 0.1813861418659316\n",
      "x3 --- -0.26492986414905223\n",
      "x4 --- -0.013856969499840707\n",
      "x5 --- -0.09757241985819824\n",
      "x6 --- -0.010884790187832904\n",
      "x7 --- -0.004341737516126053\n",
      "x8 --- 0.04777304729507007\n",
      "x9 --- 9.744137437290258e-06\n",
      "x10 --- 0.06127065178427912\n",
      "-2.4713828635564012 -6.94910795573378e-15\n",
      "7.979753732681274 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Generating versions of the original dataframe with d=10 and d=50\n",
    "df_10 = df.copy()\n",
    "df_50 = df.copy()\n",
    "\n",
    "for i in range(6, 11):\n",
    "    newx = np.random.normal(0, 1, (11000,1))\n",
    "    df_10.insert(len(df_10.columns)-1, f'x{i}', newx)\n",
    "\n",
    "for i in range(6, 51):\n",
    "    newx = np.random.normal(0, 1, (11000,1))\n",
    "    df_50.insert(len(df_50.columns)-1, f'x{i}', newx)\n",
    "\n",
    "# splitting datasets into train and test data\n",
    "df_10_train = df_10[0:10000]\n",
    "df_10_test = df_10[10000:].reset_index(drop=True)\n",
    "\n",
    "df_50_train = df_50[0:10000]\n",
    "df_50_test = df_50[10000:].reset_index(drop=True)\n",
    "\n",
    "df_10_train_x = df_10_train.iloc[:,0:-1]\n",
    "df_10_train_y = df_10_train.iloc[:,-1]\n",
    "\n",
    "df_50_train_x = df_50_train.iloc[:,0:-1]\n",
    "df_50_train_y = df_50_train.iloc[:,-1]\n",
    "\n",
    "st = time.time()\n",
    "tophead = Node(left=None, right=None, depth=0, dataframe=df_10_train)\n",
    "lx, ly, rx, ry = tophead.splitNode(df_10_train_x.head(1000), df_10_train_y.head(1000))\n",
    "end = time.time()\n",
    "print(f\"{(end-st)} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "474792d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tophead.left.splitNode(pd.DataFrame(lx),ly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c51ca08a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "deptharray = np.arange(1,50,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66013687",
   "metadata": {},
   "source": [
    "Just based on error analysis, the optimal depth seems to be around 10 nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f5c73a",
   "metadata": {},
   "source": [
    "#### 3)  Repeat the experiment, but instead of truncating the tree by depth, truncate by sample size (i.e., when the number of sample points down a branch drops below a threshold, freeze that branch).  For d= 10, generate a plot of the decision tree error on the training and testing data as the allowed sample size increases.  What appears to be the optimal sample size threshold to minimize the error?  How does the performance of this tree compare to the performance of the constant model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8722e7b8",
   "metadata": {},
   "source": [
    "The performance of the tree is definitively better than the constant model. While using a sample size too small cause the testing error to increase due to overfitting, finding the optimal sample size allows for the model to perform well on data it has never seen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227618d3",
   "metadata": {},
   "source": [
    "#### 4)  Which is better for minimizing error?  Truncating by depth or truncating by sample size?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b636b67",
   "metadata": {},
   "source": [
    "Truncating by sample size is superior here. As mostly all of the features are close to each other, more information about the output is gained by reducing and grouping the output space together rather than splitting it up. This is directly dependent of the fact that there aren't as many superfluous features, however, so they are more easily abled to be seperated out."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda1fd33",
   "metadata": {},
   "source": [
    "#### 5)  Consider repeating this experiment but now with d= 50.  Do the optimal depth and sample sizes change, based on your training and testing data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f801ee",
   "metadata": {},
   "source": [
    "As decision trees are in general, heavily susceptible to small fluctuations and random noise, the optimal depth becomes more important now. The testing error for d=50 proves this, as the error slightly increases when compared to the d=10 experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25783594",
   "metadata": {},
   "source": [
    "#### 6)  Consider repeating the above experiments for different values of d.  Plot,  as a function of d,  the number of superfluous features that are included in the decision tree (i.e., the number of variables X6,...,Xd that areincluded in the decision tree).  Which approach is better for excluding independent features?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491721b0",
   "metadata": {},
   "source": [
    "As d grows, more superfluous features are added in to the model. Being closely centered around the origin, all of these extra features are more easily filtered out through the maximum depth approach, as minimum sample size allows for the output space to be more prone to overfitting. while d is small, minimum sample size is superior, slowing shifting to maximum depth as the noise increases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c1fa8b",
   "metadata": {},
   "source": [
    "#### 7)  Write a program to take a data set and fit a linear model to it.  For d = 10, give the coefficients for your fitted model.  How does the error of your model on the testing data compare to the error of the constant model?  Is overfitting an issue here?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53a3e95",
   "metadata": {},
   "source": [
    "To create a linear model, first lets create the appropriate datasets that we'll need. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31adba57",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Generating versions of the original dataframe with d=10 and d=50\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m df_10 \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m      3\u001b[0m df_50 \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m6\u001b[39m, \u001b[38;5;241m11\u001b[39m):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# Generating versions of the original dataframe with d=10 and d=50\n",
    "df_10 = df.copy()\n",
    "df_50 = df.copy()\n",
    "\n",
    "for i in range(6, 11):\n",
    "    newx = np.random.normal(0, 1, (11000,1))\n",
    "    df_10.insert(len(df_10.columns)-1, f'x{i}', newx)\n",
    "\n",
    "for i in range(6, 51):\n",
    "    newx = np.random.normal(0, 1, (11000,1))\n",
    "    df_50.insert(len(df_50.columns)-1, f'x{i}', newx)\n",
    "\n",
    "# splitting datasets into train and test data\n",
    "df_10_train = df_10[0:10000]\n",
    "df_10_test = df_10[10000:].reset_index(drop=True)\n",
    "\n",
    "df_50_train = df_50[0:10000]\n",
    "df_50_test = df_50[10000:].reset_index(drop=True)\n",
    "\n",
    "df_10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14251996",
   "metadata": {},
   "source": [
    "We'll first try finding w_star numerically, and see how much time the computation takes for NumPy. In case it takes too long, we can implement gradient descent to find w_star instead. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b5eb6a9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.84655707e+01 -1.11511805e+01  4.75571327e+00  6.68323633e-01\n",
      "  9.69845421e+00 -8.26519694e-02  2.46411532e-04  5.28950958e-02\n",
      " -1.70350506e-01  6.78150231e-03]\n",
      "Calculation took 7.001399993896484e-06 seconds.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "st = time.time()\n",
    "\n",
    "df_x = df_10_train.iloc[:,0:-1]\n",
    "df_y = df_10_train.iloc[:,-1]\n",
    "df_x_np = df_x.to_numpy()\n",
    "df_y_np = df_y.to_numpy()\n",
    "\n",
    "#performing calculation\n",
    "multi = np.matmul(df_x_np.transpose(), df_x_np)\n",
    "multi_inv = np.linalg.inv(multi)\n",
    "multi_inv_times_trans = np.matmul(multi_inv, df_x_np.transpose())\n",
    "w_star = np.matmul(multi_inv_times_trans, df_y_np)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "\n",
    "print(w_star)\n",
    "print(f'Calculation took {(end-st)/1000} seconds.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca1fe0d",
   "metadata": {},
   "source": [
    "The calculation was staggeringly fast. Let's use our vector of weights and find our error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2878cb0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The coefficients for this model are [-18.466, -11.151, 4.756, 0.668, 9.698, -0.083, 0.0, 0.053, -0.17, 0.007]\n",
      "The average error is 6.060150713225346 on the test set.\n",
      "2.628046590424467 -139.5462489966199\n"
     ]
    }
   ],
   "source": [
    "# Converting test dataframes into numpy arrays\n",
    "df_x_test = df_10_test.iloc[:,0:-1]\n",
    "df_y_test = df_10_test.iloc[:,-1]\n",
    "df_x_np_test = df_x_test.to_numpy()\n",
    "df_y_np_test = df_y_test.to_numpy()\n",
    "\n",
    "m = df_x_np_test.shape[0]\n",
    "error_test = 0\n",
    "\n",
    "for i in range(m):\n",
    "    predicted_value = np.dot(w_star, df_x_np_test[i])\n",
    "    error += abs(predicted_value - df_y_np_test[i])\n",
    "\n",
    "error /= m\n",
    "\n",
    "print(f'The coefficients for this model are {[round(i, 3) for i in w_star.tolist()]}')\n",
    "print(f'The average error is {error} on the test set.')\n",
    "print(df_y_np_test.max(), df_y_np_test.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3681e7d5",
   "metadata": {},
   "source": [
    "The linear model found automatically by calculating w_star definitively outperformed our constant model, with an error of 6.05 as compared to 14.2, around a 57% improvement. Looking at the coefficients, x1 and x2 have the greatest weight on the outcome of y, which makes sense as x1 is squared and x2 is multiplied by a bernoulli feature. Meanwhile, the unnecessary features have much lower weights, proving that the algorithm has worked."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81a4bd7",
   "metadata": {},
   "source": [
    "#### 8)  Consider the following scheme to try to eliminate superfluous features: when you fit a model, look at the weight on each feature.  If for feature i, |wi| ≤ ep, eliminate that feature from consideration.  Whatever features remain, re-fit a linear model on those features.  For d= 50, plot the number of superfluous features that make it into the final model as a function of ep, and plot the error on the testing data for the final model as a function of ep. Is this a good strategy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "11307b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "df_x_50 = df_50_train.iloc[:,0:-1]\n",
    "df_y_50 = df_50_train.iloc[:,-1]\n",
    "df_x_np_50 = df_x_50.to_numpy()\n",
    "df_y_np_50 = df_y_50.to_numpy()\n",
    "\n",
    "def enhanced_model(x_pandas, x_dataset, y_vector, epsilon):\n",
    "    \n",
    "    # calculates w* vector\n",
    "    multi_50 = np.matmul(x_dataset.transpose(), x_dataset)\n",
    "    multi_inv_50 = np.linalg.inv(multi_50)\n",
    "    multi_inv_times_trans_50 = np.matmul(multi_inv_50, x_dataset.transpose())\n",
    "    w_star_50 = np.matmul(multi_inv_times_trans_50, y_vector)    \n",
    "    \n",
    "    #checks which features to eliminate\n",
    "    cols_to_remove = []\n",
    "    \n",
    "    for i,val in enumerate(w_star_50):\n",
    "        if abs(val)<=epsilon:\n",
    "            cols_to_remove.append(f'x{i+1}')\n",
    "    \n",
    "    #removes features, runs again\n",
    "    if len(cols_to_remove)!=0:\n",
    "        for col in cols_to_remove:\n",
    "            if col in x_pandas.columns.tolist():\n",
    "                x_pandas.drop(columns=[col], inplace=True)\n",
    "        x_numpy = x_pandas.to_numpy()\n",
    "        enhanced_model(x_pandas, x_numpy, y_vector, epsilon)\n",
    "        \n",
    "    # returns the \"relevant\" portion of dataset determined by epsilon\n",
    "    return x_pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "b169a056",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_w_vector(x_pandas, x_dataset, y_vector, epsilon):\n",
    "    multi_50 = np.matmul(x_dataset.transpose(), x_dataset)\n",
    "    multi_inv_50 = np.linalg.inv(multi_50)\n",
    "    multi_inv_times_trans_50 = np.matmul(multi_inv_50, x_dataset.transpose())\n",
    "    w_star_50 = np.matmul(multi_inv_times_trans_50, y_vector)\n",
    "    \n",
    "    xpandas = enhanced_model(x_pandas, x_dataset, y_vector, epsilon)\n",
    "    final_w = []\n",
    "    for column in xpandas.columns.tolist():\n",
    "        x = w_star_50.tolist()[int(column[1:])-1]\n",
    "        final_w.append(x)\n",
    "        \n",
    "    return final_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "59d89282",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_superfluous_features(epsilon):\n",
    "    newdf = enhanced_model(df_x_50, df_x_np_50, df_y_np_50, epsilon)\n",
    "    superfluous_features = newdf.columns.tolist()\n",
    "    for feature in ['x1','x2','x3','x4','x5']:\n",
    "        if feature in superfluous_features:\n",
    "            superfluous_features.remove(feature)\n",
    "\n",
    "    return len(superfluous_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "d58627e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_x_test_50 = df_50_test.iloc[:,0:-1]\n",
    "df_y_test_50 = df_50_test.iloc[:,-1]\n",
    "df_x_np_test_50 = df_x_test_50.to_numpy()\n",
    "df_y_np_test_50 = df_y_test_50.to_numpy()\n",
    "\n",
    "def calculate_test_error(epsilon):\n",
    "    newdf = enhanced_model(df_x_50, df_x_np_50, df_y_np_50, epsilon)\n",
    "    w_vec = get_w_vector(df_x_50, df_x_np_50, df_y_np_50, epsilon)\n",
    "    \n",
    "    cols = newdf.columns\n",
    "    df_newtest = df_x_test_50.loc[:,cols]\n",
    "    df_newtest_np = df_newtest.to_numpy()\n",
    "        \n",
    "    m = df_newtest_np.shape[0]\n",
    "    error = 0\n",
    "\n",
    "    for i in range(m):\n",
    "        predicted_value = np.dot(w_vec, df_newtest_np[i])\n",
    "        error += abs(predicted_value - df_y_np_test_50[i])\n",
    "\n",
    "    error /= m\n",
    "    \n",
    "    return(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "0e1f0433",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# reset the datasets\n",
    "df_x_50 = df_50_train.iloc[:,0:-1]\n",
    "df_y_50 = df_50_train.iloc[:,-1]\n",
    "df_x_np_50 = df_x_50.to_numpy()\n",
    "df_y_np_50 = df_y_50.to_numpy()\n",
    "\n",
    "# create arrays \n",
    "epsilon_array = np.arange(0, 0.35, 0.01).tolist()\n",
    "epsilon_array.remove(0.09) # my function doesn't work for epsilon values ending in 9 for some reason\n",
    "epsilon_array.remove(0.19)\n",
    "epsilon_array.remove(0.29)\n",
    "super_features = []\n",
    "error_array = []\n",
    "\n",
    "for epsilon in epsilon_array:\n",
    "    super_features.append(check_superfluous_features(epsilon))\n",
    "    error_array.append(calculate_test_error(epsilon))\n",
    "\n",
    "\n",
    "# plot the graphs\n",
    "plt.plot(epsilon_array, super_features)\n",
    "plt.title(\"Epsilon vs number of extra features\")\n",
    "plt.xlabel(\"Epsilon\")\n",
    "plt.ylabel(\"number of extra features\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "a9c6b429",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(epsilon_array, error_array)\n",
    "plt.title(\"Epsilon vs average error\")\n",
    "plt.xlabel(\"Epsilon\")\n",
    "plt.ylabel(\"average error\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c66f1d",
   "metadata": {},
   "source": [
    "In the end, the number of extra features went down as epsilon increased, disappearing around epsilon=0.3. The error did not change much with regards to epsilon, which makes sense as having small weights on a feature necessarily means that it does not affect the model much. The sharp increase at the beginning of the graph can be explained purely by randomness, as the model is unsure about whether or not the features are relevant. The graph slopes horizontally by the end, as almost all of the unnecessary features are eliminated.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39173291",
   "metadata": {},
   "source": [
    "#### 9)  Which model is superior here?  Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db89c1bf",
   "metadata": {},
   "source": [
    "The second linear model in question 8 is better. First, the error is very slightly lower. More importantly, however, by truncating unnecessary features, the model focuses more on fitting to features that are more relevant to y, essentially eliminating random noise to more definitively predict the output. This is also the idea behind Lasso regression, which uses a different algorithm but has a similar idea behind it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "46764ac3a747c7ca73c4630527f610bbe3bb23a5d08b09a3ce898d5bc25494fc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
